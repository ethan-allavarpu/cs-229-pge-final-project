{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import plotly\n",
    "import tqdm as notebook_tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv(\n",
    "    \"../data/processed/x_train_w_OHE.csv\", index_col=0, dtype=str\n",
    ")\n",
    "x_test = pd.read_csv(\n",
    "    \"../data/processed/x_test_w_OHE.csv\", index_col=0, dtype=str\n",
    ")\n",
    "y_train = pd.read_csv(\n",
    "    \"../data/processed/y_train.csv\", index_col=0, dtype=float\n",
    ").squeeze(\"columns\").reset_index(drop=True)\n",
    "y_test = pd.read_csv(\n",
    "    \"../data/processed/y_test.csv\", index_col=0, dtype=float\n",
    ").squeeze(\"columns\").reset_index(drop=True)\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    x_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>circuit_name</th>\n",
       "      <th>deenergize_time</th>\n",
       "      <th>restoration_time</th>\n",
       "      <th>key_communities</th>\n",
       "      <th>hftd_tier</th>\n",
       "      <th>total_affected</th>\n",
       "      <th>residential_affected</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>...</th>\n",
       "      <th>zip_is_96035</th>\n",
       "      <th>zip_is_96051</th>\n",
       "      <th>zip_is_96055</th>\n",
       "      <th>zip_is_96059</th>\n",
       "      <th>zip_is_96069</th>\n",
       "      <th>zip_is_96073</th>\n",
       "      <th>zip_is_96076</th>\n",
       "      <th>zip_is_96080</th>\n",
       "      <th>zip_is_96096</th>\n",
       "      <th>zip_is_96137</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>SWIFT2110</td>\n",
       "      <td>2019-10-10 00:05:00</td>\n",
       "      <td>2019-10-11 14:17:00</td>\n",
       "      <td>SAN JOSE, LIVERMORE</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2232.0</td>\n",
       "      <td>2047.0</td>\n",
       "      <td>95148</td>\n",
       "      <td>-121.796959988177</td>\n",
       "      <td>37.3225680192999</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>STATION E EUREKA1105</td>\n",
       "      <td>2019-10-09 01:20:00</td>\n",
       "      <td>2019-10-09 23:10:00</td>\n",
       "      <td>EUREKA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1618.0</td>\n",
       "      <td>1264.0</td>\n",
       "      <td>95501</td>\n",
       "      <td>-124.180313467792</td>\n",
       "      <td>40.7934324220744</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804</th>\n",
       "      <td>SILVERADO</td>\n",
       "      <td>2021-08-17 18:22:00</td>\n",
       "      <td>2021-08-18 23:55:00</td>\n",
       "      <td>NAPA</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1815.0</td>\n",
       "      <td>1516.0</td>\n",
       "      <td>94574</td>\n",
       "      <td>-122.459110675812</td>\n",
       "      <td>38.4998179385502</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>OREGON TRAIL1103</td>\n",
       "      <td>2019-10-09 01:39:00</td>\n",
       "      <td>2019-10-11 11:32:00</td>\n",
       "      <td>REDDING, BELLA VISTA</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1706.0</td>\n",
       "      <td>1599.0</td>\n",
       "      <td>96003</td>\n",
       "      <td>-122.322060242415</td>\n",
       "      <td>40.619045588007</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>OREGON TRAIL</td>\n",
       "      <td>2020-10-22 03:23:00</td>\n",
       "      <td>2020-10-23 11:30:00</td>\n",
       "      <td>PALO CEDRO, REDDING</td>\n",
       "      <td>2.0</td>\n",
       "      <td>952.0</td>\n",
       "      <td>843.0</td>\n",
       "      <td>96003</td>\n",
       "      <td>-122.322060242415</td>\n",
       "      <td>40.619045588007</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 284 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              circuit_name      deenergize_time     restoration_time  \\\n",
       "452              SWIFT2110  2019-10-10 00:05:00  2019-10-11 14:17:00   \n",
       "443   STATION E EUREKA1105  2019-10-09 01:20:00  2019-10-09 23:10:00   \n",
       "1804             SILVERADO  2021-08-17 18:22:00  2021-08-18 23:55:00   \n",
       "340       OREGON TRAIL1103  2019-10-09 01:39:00  2019-10-11 11:32:00   \n",
       "1390          OREGON TRAIL  2020-10-22 03:23:00  2020-10-23 11:30:00   \n",
       "\n",
       "           key_communities hftd_tier total_affected residential_affected  \\\n",
       "452    SAN JOSE, LIVERMORE       2.0         2232.0               2047.0   \n",
       "443                 EUREKA       0.0         1618.0               1264.0   \n",
       "1804                  NAPA       3.0         1815.0               1516.0   \n",
       "340   REDDING, BELLA VISTA       2.0         1706.0               1599.0   \n",
       "1390   PALO CEDRO, REDDING       2.0          952.0                843.0   \n",
       "\n",
       "     zip_code          longitude          latitude  ... zip_is_96035  \\\n",
       "452     95148  -121.796959988177  37.3225680192999  ...            0   \n",
       "443     95501  -124.180313467792  40.7934324220744  ...            0   \n",
       "1804    94574  -122.459110675812  38.4998179385502  ...            0   \n",
       "340     96003  -122.322060242415   40.619045588007  ...            0   \n",
       "1390    96003  -122.322060242415   40.619045588007  ...            0   \n",
       "\n",
       "     zip_is_96051 zip_is_96055 zip_is_96059 zip_is_96069 zip_is_96073  \\\n",
       "452             0            0            0            0            0   \n",
       "443             0            0            0            0            0   \n",
       "1804            0            0            0            0            0   \n",
       "340             0            0            0            0            0   \n",
       "1390            0            0            0            0            0   \n",
       "\n",
       "     zip_is_96076 zip_is_96080 zip_is_96096 zip_is_96137  \n",
       "452             0            0            0            0  \n",
       "443             0            0            0            0  \n",
       "1804            0            0            0            0  \n",
       "340             0            0            0            0  \n",
       "1390            0            0            0            0  \n",
       "\n",
       "[5 rows x 284 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_cols = x_train.columns[\n",
    "    [re.search('zip_is', col) is not None for col in x_train.columns]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_types_x(df, numeric_cols):\n",
    "    for col in ['deenergize_time', 'restoration_time']:\n",
    "        df[col] = pd.to_datetime(df[col], format='%Y-%m-%d %H:%M:%S')\n",
    "    for col in numeric_cols:\n",
    "        df[col] = df[col].astype(float)\n",
    "    return df\n",
    "\n",
    "\n",
    "numeric_cols = [\n",
    "    'hftd_tier', 'total_affected', 'residential_affected',\n",
    "    'longitude', 'latitude', 'total_pop', 'median_age', 'median_income',\n",
    "    'white_pct', 'tmin_d-5', 'tmax_d-5', 'wspd_d-5', 'tmin_d-4', 'tmax_d-4',\n",
    "    'wspd_d-4', 'tmin_d-3', 'tmax_d-3', 'wspd_d-3', 'tmin_d-2', 'tmax_d-2',\n",
    "    'wspd_d-2', 'tmin_d-1', 'tmax_d-1', 'wspd_d-1'\n",
    "]\n",
    "x_train = get_correct_types_x(x_train, numeric_cols)\n",
    "x_valid = get_correct_types_x(x_valid, numeric_cols)\n",
    "x_test = get_correct_types_x(x_test, numeric_cols)\n",
    "rel_x_train = x_train[numeric_cols]\n",
    "rel_x_valid = x_valid[numeric_cols]\n",
    "rel_x_test = x_test[numeric_cols]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(rel_x_train)\n",
    "scaled_x_train = scaler.transform(rel_x_train)\n",
    "scaled_x_valid = scaler.transform(rel_x_valid)\n",
    "scaled_x_test = scaler.transform(rel_x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class base_model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_hidden_layers, n_hidden_units, activation=torch.nn.ReLU()):\n",
    "        super(base_model, self).__init__()\n",
    "        if n_hidden_layers == 0:\n",
    "            self.linears =torch.nn.ModuleList([\n",
    "                torch.nn.Linear(scaled_x_train.shape[1], 1)\n",
    "            ])\n",
    "            self.activation = activation\n",
    "        else:\n",
    "            assert len(n_hidden_units) == n_hidden_layers\n",
    "            self.layers = []\n",
    "            \n",
    "            for layer, n_units in enumerate(n_hidden_units):\n",
    "                if layer == 0:\n",
    "                    curr_layer = torch.nn.Linear(scaled_x_train.shape[1], n_units)\n",
    "                else:\n",
    "                    curr_layer = torch.nn.Linear(n_hidden_units[layer - 1], n_units)\n",
    "                self.layers.append(curr_layer)\n",
    "            self.layers.append(torch.nn.Linear(n_hidden_units[-1], 1))\n",
    "            self.linears = torch.nn.ModuleList(self.layers)\n",
    "            self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.linears:\n",
    "            x = self.activation(layer(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.from_numpy(scaled_x_train).float()\n",
    "# y = torch.from_numpy(y_train.values.reshape(-1, 1)).float()\n",
    "\n",
    "# inputs = Variable(x)\n",
    "# targets = Variable(y)\n",
    "\n",
    "# # base = base_model(1, [1], activation=torch.nn.Tanh())\n",
    "# base = base_model(2, [6, 3])\n",
    "# print(base)\n",
    "# optimizer = torch.optim.Adagrad(base.parameters(), lr=0.2)\n",
    "# loss_func = torch.nn.MSELoss()\n",
    "\n",
    "# for i in range(100000):\n",
    "#    prediction = base(inputs)\n",
    "#    loss = loss_func(prediction, targets)\n",
    "#    if i % 100 == 0:\n",
    "#       print(loss)\n",
    "#    optimizer.zero_grad()\n",
    "#    loss.backward()\n",
    "#    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Used TanH instead of ReLU\n",
    "- Adagrad instead of SGD -> SGD just returned 0 for all predictions\n",
    "- More layers -> more overfitting, run simpler networks for more epochs gets better test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series(prediction.detach().numpy().reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sqrt(loss.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_x = Variable(torch.from_numpy(scaled_x_test).float())\n",
    "# test_y = Variable(torch.from_numpy(y_test.values.reshape(-1, 1)).float())\n",
    "# test_predictions = base(test_x)\n",
    "# valid_x = Variable(torch.from_numpy(scaled_x_valid).float())\n",
    "# valid_y = Variable(torch.from_numpy(y_valid.values.reshape(-1, 1)).float())\n",
    "# valid_predictions = base(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = loss_func(valid_predictions, valid_y)\n",
    "# print(np.sqrt(loss.detach().numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "\n",
    "    # 2. Suggest values of the hyperparameters using a trial object.\n",
    "    n_layers = trial.suggest_int('n_layers', 0, 3)\n",
    "    n_hidden_units = [0] * n_layers\n",
    "    print(n_layers)\n",
    "    for i in range(n_layers):\n",
    "        n_hidden_units[i] = trial.suggest_int(f\"n_h_{i}\", 1, 100)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 5e-1, log=True)\n",
    "    n_epochs = trial.suggest_int(\"n_epochs\", 1000, 100000)\n",
    "    print(f\"\"\"Params:\n",
    "          n_layers: {n_layers}\n",
    "          n_hidden_units: {n_hidden_units}\n",
    "          lr: {lr}\n",
    "          n_epochs: {n_epochs}\"\"\")\n",
    "        \n",
    "    x = torch.from_numpy(scaled_x_train).float()\n",
    "    y = torch.from_numpy(y_train.values.reshape(-1, 1)).float()\n",
    "\n",
    "    inputs = Variable(x)\n",
    "    targets = Variable(y)\n",
    "\n",
    "    # base = base_model(1, [1], activation=torch.nn.Tanh())\n",
    "    base = base_model(n_layers, n_hidden_units)\n",
    "    optimizer = torch.optim.Adagrad(base.parameters(), lr=lr)\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        prediction = base(inputs)\n",
    "        loss = loss_func(prediction, targets)\n",
    "        if i % 1000 == 0:\n",
    "            print(loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "    valid_x = Variable(torch.from_numpy(scaled_x_valid).float())\n",
    "    valid_y = Variable(torch.from_numpy(y_valid.values.reshape(-1, 1)).float())\n",
    "    valid_predictions = base(valid_x)\n",
    "    loss = loss_func(valid_predictions, valid_y)\n",
    "    print(f\"Final valid loss: {loss}\")\n",
    "    print(\"#################\")\n",
    "    return np.sqrt(loss.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create a study object and optimize the objective function.\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "pd.DataFrame.from_dict({\"value\": study.best_trial.values, \"params\": str(\n",
    "    study.best_trial.params)}).to_csv(\"nn_hpo/run_1.csv\", index=False)\n",
    "\n",
    "fig = optuna.visualization.plot_optimization_history(study)\n",
    "# fig.show()\n",
    "fig.write_image(\"nn_hpo/run_1.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenTrial(number=34, values=[869.7608032226562], datetime_start=datetime.datetime(2022, 11, 25, 14, 54, 52, 398277), datetime_complete=datetime.datetime(2022, 11, 25, 14, 55, 24, 652445), params={'n_layers': 2, 'n_h_0': 25, 'n_h_1': 70, 'lr': 0.0065996011510886, 'n_epochs': 75122}, distributions={'n_layers': IntDistribution(high=3, log=False, low=0, step=1), 'n_h_0': IntDistribution(high=100, log=False, low=1, step=1), 'n_h_1': IntDistribution(high=100, log=False, low=1, step=1), 'lr': FloatDistribution(high=0.5, log=True, low=1e-05, step=None), 'n_epochs': IntDistribution(high=100000, log=False, low=1000, step=1)}, user_attrs={}, system_attrs={}, intermediate_values={}, trial_id=34, state=TrialState.COMPLETE, value=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95ef66565be0ab37caf8d705f94b2ba6fd3ff1d5242a0930694c635ab854b36c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
