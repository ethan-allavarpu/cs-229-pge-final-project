{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1522a2050>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv(\n",
    "    \"../data/processed/x_train_w_OHE.csv\", index_col=0, dtype=str\n",
    ")\n",
    "x_test = pd.read_csv(\n",
    "    \"../data/processed/x_test_w_OHE.csv\", index_col=0, dtype=str\n",
    ")\n",
    "y_train = pd.read_csv(\n",
    "    \"../data/processed/y_train.csv\", index_col=0, dtype=float\n",
    ").squeeze(\"columns\").reset_index(drop=True)\n",
    "y_test = pd.read_csv(\n",
    "    \"../data/processed/y_test.csv\", index_col=0, dtype=float\n",
    ").squeeze(\"columns\").reset_index(drop=True)\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "zip_cols = x_train.columns[\n",
    "    [re.search('zip_is', col) is not None for col in x_train.columns]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_types_x(df, numeric_cols):\n",
    "    for col in ['deenergize_time', 'restoration_time']:\n",
    "        df[col] = pd.to_datetime(df[col], format='%Y-%m-%d %H:%M:%S')\n",
    "    for col in numeric_cols:\n",
    "        df[col] = df[col].astype(float)\n",
    "    return df\n",
    "\n",
    "# Only pick out numeric columns\n",
    "numeric_cols = [\n",
    "    'hftd_tier', 'total_affected', 'residential_affected',\n",
    "    'longitude', 'latitude', 'total_pop', 'median_age', 'median_income',\n",
    "    'white_pct', 'tmin_d-5', 'tmax_d-5', 'wspd_d-5', 'tmin_d-4', 'tmax_d-4',\n",
    "    'wspd_d-4', 'tmin_d-3', 'tmax_d-3', 'wspd_d-3', 'tmin_d-2', 'tmax_d-2',\n",
    "    'wspd_d-2', 'tmin_d-1', 'tmax_d-1', 'wspd_d-1', 'day_in_year'\n",
    "]\n",
    "x_train = get_correct_types_x(x_train, numeric_cols)\n",
    "x_valid = get_correct_types_x(x_valid, numeric_cols)\n",
    "x_test = get_correct_types_x(x_test, numeric_cols)\n",
    "rel_x_train = x_train[numeric_cols]\n",
    "rel_x_valid = x_valid[numeric_cols]\n",
    "rel_x_test = x_test[numeric_cols]\n",
    "\n",
    "# Scale numeric columns to prevent feature dominance\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(rel_x_train)\n",
    "scaled_x_train = scaler.transform(rel_x_train)\n",
    "scaled_x_valid = scaler.transform(rel_x_valid)\n",
    "scaled_x_test = scaler.transform(rel_x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class base_model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_hidden_layers, n_hidden_units, p=0.1, activation=torch.nn.ReLU()):\n",
    "        super(base_model, self).__init__()\n",
    "        # Handle variable number of layers\n",
    "        if n_hidden_layers == 0:\n",
    "            # If 0 hidden layers, then only have 1 layer going from input to single node\n",
    "            self.linears = torch.nn.ModuleList([\n",
    "                torch.nn.Linear(scaled_x_train.shape[1], 1)\n",
    "            ])\n",
    "            self.activation = activation\n",
    "            self.dropout = torch.nn.Dropout(p)\n",
    "        else:\n",
    "            # If >= 1 hidden layer, then specify # of hidden units per layer\n",
    "            assert len(n_hidden_units) == n_hidden_layers\n",
    "            self.layers = []\n",
    "\n",
    "            for layer, n_units in enumerate(n_hidden_units):\n",
    "                if layer == 0:\n",
    "                    curr_layer = torch.nn.Linear(\n",
    "                        scaled_x_train.shape[1], n_units)\n",
    "                else:\n",
    "                    curr_layer = torch.nn.Linear(\n",
    "                        n_hidden_units[layer - 1], n_units)\n",
    "                self.layers.append(curr_layer)\n",
    "            # Add layers to module list\n",
    "            self.layers.append(torch.nn.Linear(n_hidden_units[-1], 1))\n",
    "            self.linears = torch.nn.ModuleList(self.layers)\n",
    "            self.activation = activation\n",
    "            self.dropout = torch.nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.linears:\n",
    "            x = self.dropout(self.activation(layer(x)))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter Optimization using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "\n",
    "    # Suggest values of the hyperparameters using a trial object.\n",
    "    n_layers = trial.suggest_int('n_layers', 0, 4)\n",
    "    n_hidden_units = [0] * n_layers\n",
    "    print(n_layers)\n",
    "    for i in range(n_layers):\n",
    "        n_hidden_units[i] = trial.suggest_int(f\"n_h_{i}\", 1, 50)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 5e-1, log=True)\n",
    "    n_epochs = trial.suggest_int(\"n_epochs\", 1000, 100000)\n",
    "    activation_functions = [torch.nn.ReLU(), torch.nn.Tanh()]\n",
    "    act_idx = trial.suggest_categorical(\"act_function\", [0, 1])\n",
    "    dropout_p = trial.suggest_float(\"dropout\", 0, 1)\n",
    "    params = f\"\"\"Params:\n",
    "          n_layers: {n_layers}\n",
    "          n_hidden_units: {n_hidden_units}\n",
    "          lr: {lr}\n",
    "          n_epochs: {n_epochs}\n",
    "          act_function: {activation_functions[act_idx]}\n",
    "          dropout: {dropout_p}\"\"\"\n",
    "    print(params)\n",
    "    \n",
    "    # Move data to tensors\n",
    "    x = torch.from_numpy(scaled_x_train).float().to(device)\n",
    "    y = torch.from_numpy(y_train.values.reshape(-1, 1)).float().to(device)\n",
    "    inputs = Variable(x)\n",
    "    targets = Variable(y)\n",
    "    \n",
    "    # Define model, optimizer with hyperparameters\n",
    "    base = base_model(n_layers, n_hidden_units, p=dropout_p,\n",
    "                      activation=activation_functions[act_idx])\n",
    "    base.to(device)\n",
    "    optimizer = torch.optim.Adagrad(base.parameters(), lr=lr)\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    prev_loss = torch.from_numpy(np.array([0])).to(device)\n",
    "\n",
    "    # Train model for n_epochs\n",
    "    for i in range(n_epochs):\n",
    "        prediction = base(inputs)\n",
    "        loss = loss_func(prediction, targets)\n",
    "        if i % 1000 == 0:\n",
    "            print(loss)\n",
    "        if np.abs(loss.cpu().detach().numpy() - prev_loss.cpu().detach().numpy()) < 1e-8:\n",
    "          break\n",
    "        prev_loss = loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Calculate validation loss\n",
    "    valid_x = Variable(torch.from_numpy(scaled_x_valid).float()).to(device)\n",
    "    valid_y = Variable(torch.from_numpy(\n",
    "        y_valid.values.reshape(-1, 1)).float()).to(device)\n",
    "    valid_predictions = base(valid_x)\n",
    "    loss = loss_func(valid_predictions, valid_y)\n",
    "    print(f\"Final valid loss: {loss}\")\n",
    "    print(\"#################\")\n",
    "    if loss < 1000000:\n",
    "      with open(f\"nn_hpo/run_2/{time.time()}.txt\", \"w+\") as f:\n",
    "        f.write(f\"Loss: {np.sqrt(loss.cpu().detach().numpy())}\\n\")\n",
    "        f.write(params)\n",
    "    # Return validation RMSE for optuna to minimize\n",
    "    return np.sqrt(loss.cpu().detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a study object and minimize RMSE\n",
    "torch.manual_seed(0)\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "# Save best trial with parameters\n",
    "pd.DataFrame.from_dict({\"value\": study.best_trial.values, \"params\": str(\n",
    "    study.best_trial.params)}).to_csv(\"nn_hpo/run_4.csv\", index=False)\n",
    "\n",
    "# fig = optuna.visualization.plot_optimization_history(study)\n",
    "# fig.show()\n",
    "# fig.write_image(\"gdrive/MyDrive/CS229_Final_Project/nn_hpo/run_1.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running best models on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_layers': 2,\n",
       " 'n_h_0': 46,\n",
       " 'n_h_1': 96,\n",
       " 'lr': 0.011578444404576697,\n",
       " 'n_epochs': 44718,\n",
       " 'act_function': 0,\n",
       " 'dropout': 0.05}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load best hyperparams\n",
    "best_params = pd.read_csv(\"nn_hpo/run_1.csv\")\n",
    "best_params_dict = eval(best_params[\"params\"].values[0])\n",
    "best_params_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(scaled_x_train).float()\n",
    "y = torch.from_numpy(y_train.values.reshape(-1, 1)).float()\n",
    "\n",
    "inputs = Variable(x)\n",
    "targets = Variable(y)\n",
    "\n",
    "base = base_model(1, [20])\n",
    "print(base)\n",
    "optimizer = torch.optim.Adagrad(base.parameters())\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "for i in range(100000):\n",
    "   prediction = base(inputs)\n",
    "   loss_base = loss_func(prediction, targets)\n",
    "   if i % 1000 == 0:\n",
    "      print(loss_base)\n",
    "   optimizer.zero_grad()\n",
    "   loss_base.backward()\n",
    "   optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1527.9476"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline loss\n",
    "test_x = Variable(torch.from_numpy(scaled_x_test).float())\n",
    "test_y = Variable(torch.from_numpy(y_test.values.reshape(-1, 1)).float())\n",
    "test_predictions_base = base(test_x)\n",
    "loss_base = loss_func(test_predictions_base, test_y)\n",
    "baseline_rmse = np.sqrt(loss_base.detach().numpy())\n",
    "baseline_rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(scaled_x_train).float()\n",
    "y = torch.from_numpy(y_train.values.reshape(-1, 1)).float()\n",
    "\n",
    "inputs = Variable(x)\n",
    "targets = Variable(y)\n",
    "\n",
    "# Load model, optimizer with best hyperparameters\n",
    "best = base_model(best_params_dict[\"n_layers\"], \n",
    "                  [46, 96],\n",
    "                  # activation=best_params_dict[\"act_function\"],\n",
    "                  p=best_params_dict[\"dropout\"]\n",
    ")\n",
    "print(best)\n",
    "optimizer = torch.optim.Adagrad(best.parameters(), lr=best_params_dict[\"lr\"])\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "for i in range(best_params_dict[\"n_epochs\"]):\n",
    "   prediction = best(inputs)\n",
    "   loss = loss_func(prediction, targets)\n",
    "   if i % 1000 == 0:\n",
    "      print(loss)\n",
    "   optimizer.zero_grad()\n",
    "   loss.backward()\n",
    "   optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "821.0526"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best model loss\n",
    "test_predictions = best(test_x)\n",
    "loss = loss_func(test_predictions, test_y)\n",
    "best_rmse = np.sqrt(loss.detach().numpy())\n",
    "best_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_test_r2(pred_vals, true_vals, baseline_rmse):\n",
    "    sse = mean_squared_error(pred_vals, true_vals) * len(true_vals)\n",
    "    sst = (baseline_rmse ** 2) * len(true_vals)\n",
    "    return (\n",
    "        1 - sse / sst, \n",
    "        np.sqrt(sse / len(true_vals)),\n",
    "        mean_absolute_error(pred_vals, true_vals),\n",
    "        mean_absolute_percentage_error(pred_vals, true_vals)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R-Squared: 0.8583690037411281\n",
      "RMSE: 531.1011694262464\n",
      "MAE: 376.44102060111464\n",
      "MAPE: 0.14951429283324544\n",
      "Test R-Squared: 0.7299565495760196\n",
      "RMSE: 821.0526518534082\n",
      "MAE: 590.262234054449\n",
      "MAPE: 0.2346296805366815\n"
     ]
    }
   ],
   "source": [
    "train_r2, rmse, mae, mape = calc_test_r2(\n",
    "    prediction.detach().numpy(), y_train.values.reshape(-1, 1), np.sqrt(np.var(y_train)))\n",
    "print('Train R-Squared:', train_r2)\n",
    "print('RMSE:', rmse)\n",
    "print('MAE:', mae)\n",
    "print('MAPE:', mape)\n",
    "\n",
    "test_r2, rmse, mae, mape = calc_test_r2(\n",
    "    test_predictions.detach().numpy(), y_test.values.reshape(-1, 1), np.sqrt(np.var(y_test)))\n",
    "print('Test R-Squared:', test_r2)\n",
    "print('RMSE:', rmse)\n",
    "print('MAE:', mae)\n",
    "print('MAPE:', mape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "95ef66565be0ab37caf8d705f94b2ba6fd3ff1d5242a0930694c635ab854b36c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
