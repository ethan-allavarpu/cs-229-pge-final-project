{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1291c64f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import plotly\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv(\n",
    "    \"../data/processed/x_train_w_OHE.csv\", index_col=0, dtype=str\n",
    ")\n",
    "x_test = pd.read_csv(\n",
    "    \"../data/processed/x_test_w_OHE.csv\", index_col=0, dtype=str\n",
    ")\n",
    "y_train = pd.read_csv(\n",
    "    \"../data/processed/y_train.csv\", index_col=0, dtype=float\n",
    ").squeeze(\"columns\").reset_index(drop=True)\n",
    "y_test = pd.read_csv(\n",
    "    \"../data/processed/y_test.csv\", index_col=0, dtype=float\n",
    ").squeeze(\"columns\").reset_index(drop=True)\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "zip_cols = x_train.columns[\n",
    "    [re.search('zip_is', col) is not None for col in x_train.columns]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_types_x(df, numeric_cols):\n",
    "    for col in ['deenergize_time', 'restoration_time']:\n",
    "        df[col] = pd.to_datetime(df[col], format='%Y-%m-%d %H:%M:%S')\n",
    "    for col in numeric_cols:\n",
    "        df[col] = df[col].astype(float)\n",
    "    return df\n",
    "\n",
    "\n",
    "numeric_cols = [\n",
    "    'hftd_tier', 'total_affected', 'residential_affected',\n",
    "    'longitude', 'latitude', 'total_pop', 'median_age', 'median_income',\n",
    "    'white_pct', 'tmin_d-5', 'tmax_d-5', 'wspd_d-5', 'tmin_d-4', 'tmax_d-4',\n",
    "    'wspd_d-4', 'tmin_d-3', 'tmax_d-3', 'wspd_d-3', 'tmin_d-2', 'tmax_d-2',\n",
    "    'wspd_d-2', 'tmin_d-1', 'tmax_d-1', 'wspd_d-1', 'day_in_year'\n",
    "]\n",
    "x_train = get_correct_types_x(x_train, numeric_cols)\n",
    "x_valid = get_correct_types_x(x_valid, numeric_cols)\n",
    "x_test = get_correct_types_x(x_test, numeric_cols)\n",
    "rel_x_train = x_train[numeric_cols]\n",
    "rel_x_valid = x_valid[numeric_cols]\n",
    "rel_x_test = x_test[numeric_cols]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(rel_x_train)\n",
    "scaled_x_train = scaler.transform(rel_x_train)\n",
    "scaled_x_valid = scaler.transform(rel_x_valid)\n",
    "scaled_x_test = scaler.transform(rel_x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class base_model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_hidden_layers, n_hidden_units, p=0.5, activation=torch.nn.ReLU()):\n",
    "        super(base_model, self).__init__()\n",
    "        if n_hidden_layers == 0:\n",
    "            self.linears = torch.nn.ModuleList([\n",
    "                torch.nn.Linear(scaled_x_train.shape[1], 1)\n",
    "            ])\n",
    "            self.activation = activation\n",
    "            self.dropout = torch.nn.Dropout(p)\n",
    "        else:\n",
    "            assert len(n_hidden_units) == n_hidden_layers\n",
    "            self.layers = []\n",
    "\n",
    "            for layer, n_units in enumerate(n_hidden_units):\n",
    "                if layer == 0:\n",
    "                    curr_layer = torch.nn.Linear(\n",
    "                        scaled_x_train.shape[1], n_units)\n",
    "                else:\n",
    "                    curr_layer = torch.nn.Linear(\n",
    "                        n_hidden_units[layer - 1], n_units)\n",
    "                self.layers.append(curr_layer)\n",
    "            self.layers.append(torch.nn.Linear(n_hidden_units[-1], 1))\n",
    "            self.linears = torch.nn.ModuleList(self.layers)\n",
    "            self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.linears:\n",
    "            x = self.activation(layer(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training script\n",
    "\n",
    "# x = torch.from_numpy(scaled_x_train).float()\n",
    "# y = torch.from_numpy(y_train.values.reshape(-1, 1)).float()\n",
    "\n",
    "# inputs = Variable(x)\n",
    "# targets = Variable(y)\n",
    "\n",
    "# # base = base_model(1, [1], activation=torch.nn.Tanh())\n",
    "# base = base_model(2, [6, 3])\n",
    "# print(base)\n",
    "# optimizer = torch.optim.Adagrad(base.parameters(), lr=0.2)\n",
    "# loss_func = torch.nn.MSELoss()\n",
    "\n",
    "# for i in range(100000):\n",
    "#    prediction = base(inputs)\n",
    "#    loss = loss_func(prediction, targets)\n",
    "#    if i % 100 == 0:\n",
    "#       print(loss)\n",
    "#    optimizer.zero_grad()\n",
    "#    loss.backward()\n",
    "#    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation performance\n",
    "\n",
    "# valid_x = Variable(torch.from_numpy(scaled_x_valid).float())\n",
    "# valid_y = Variable(torch.from_numpy(y_valid.values.reshape(-1, 1)).float())\n",
    "# valid_predictions = base(valid_x)\n",
    "# valid_loss = loss_func(valid_predictions, valid_y)\n",
    "# print(np.sqrt(valid_loss.detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter Optimization using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "\n",
    "    # 2. Suggest values of the hyperparameters using a trial object.\n",
    "    n_layers = trial.suggest_int('n_layers', 0, 3)\n",
    "    n_hidden_units = [0] * n_layers\n",
    "    print(n_layers)\n",
    "    for i in range(n_layers):\n",
    "        n_hidden_units[i] = trial.suggest_int(f\"n_h_{i}\", 1, 100)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 5e-1, log=True)\n",
    "    n_epochs = trial.suggest_int(\"n_epochs\", 1000, 100000)\n",
    "    print(f\"\"\"Params:\n",
    "          n_layers: {n_layers}\n",
    "          n_hidden_units: {n_hidden_units}\n",
    "          lr: {lr}\n",
    "          n_epochs: {n_epochs}\"\"\")\n",
    "        \n",
    "    x = torch.from_numpy(scaled_x_train).float()\n",
    "    y = torch.from_numpy(y_train.values.reshape(-1, 1)).float()\n",
    "\n",
    "    inputs = Variable(x)\n",
    "    targets = Variable(y)\n",
    "\n",
    "    # base = base_model(1, [1], activation=torch.nn.Tanh())\n",
    "    base = base_model(n_layers, n_hidden_units)\n",
    "    optimizer = torch.optim.Adagrad(base.parameters(), lr=lr)\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        prediction = base(inputs)\n",
    "        loss = loss_func(prediction, targets)\n",
    "        if i % 1000 == 0:\n",
    "            print(loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "    valid_x = Variable(torch.from_numpy(scaled_x_valid).float())\n",
    "    valid_y = Variable(torch.from_numpy(y_valid.values.reshape(-1, 1)).float())\n",
    "    valid_predictions = base(valid_x)\n",
    "    loss = loss_func(valid_predictions, valid_y)\n",
    "    print(f\"Final valid loss: {loss}\")\n",
    "    print(\"#################\")\n",
    "    return np.sqrt(loss.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create a study object and optimize the objective function.\n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective, n_trials=50)\n",
    "# pd.DataFrame.from_dict({\"value\": study.best_trial.values, \"params\": str(\n",
    "#     study.best_trial.params)}).to_csv(\"nn_hpo/run_1.csv\", index=False)\n",
    "\n",
    "# fig = optuna.visualization.plot_optimization_history(study)\n",
    "# fig.show()\n",
    "# fig.write_image(\"nn_hpo/run_3.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running best models on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_layers': 2,\n",
       " 'n_h_0': 46,\n",
       " 'n_h_1': 96,\n",
       " 'lr': 0.011578444404576697,\n",
       " 'n_epochs': 44718,\n",
       " 'act_function': 0,\n",
       " 'dropout': 0.05}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load best hyperparams\n",
    "best_params = pd.read_csv(\"nn_hpo/run_1.csv\")\n",
    "best_params_dict = eval(best_params[\"params\"].values[0])\n",
    "best_params_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model(\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=25, out_features=20, bias=True)\n",
      "    (1): Linear(in_features=20, out_features=1, bias=True)\n",
      "  )\n",
      "  (activation): ReLU()\n",
      ")\n",
      "tensor(9497114., grad_fn=<MseLossBackward0>)\n",
      "tensor(9137476., grad_fn=<MseLossBackward0>)\n",
      "tensor(8777967., grad_fn=<MseLossBackward0>)\n",
      "tensor(8436327., grad_fn=<MseLossBackward0>)\n",
      "tensor(8112143.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(7804646., grad_fn=<MseLossBackward0>)\n",
      "tensor(7512988.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(7236371.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(6973818.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(6724475., grad_fn=<MseLossBackward0>)\n",
      "tensor(6487451., grad_fn=<MseLossBackward0>)\n",
      "tensor(6261981.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(6047370., grad_fn=<MseLossBackward0>)\n",
      "tensor(5842989.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(5648168.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(5462438.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(5285236., grad_fn=<MseLossBackward0>)\n",
      "tensor(5115882., grad_fn=<MseLossBackward0>)\n",
      "tensor(4953852.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(4798889.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(4650756.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(4509098.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(4373616.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(4243971., grad_fn=<MseLossBackward0>)\n",
      "tensor(4119988.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(4001468.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3888212., grad_fn=<MseLossBackward0>)\n",
      "tensor(3780071.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3676673.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3578031., grad_fn=<MseLossBackward0>)\n",
      "tensor(3483957.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(3394262.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3308805.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3227432.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3149952., grad_fn=<MseLossBackward0>)\n",
      "tensor(3076113.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3005753., grad_fn=<MseLossBackward0>)\n",
      "tensor(2938754., grad_fn=<MseLossBackward0>)\n",
      "tensor(2874981.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(2814320., grad_fn=<MseLossBackward0>)\n",
      "tensor(2756608.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(2701646.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(2649296.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(2599422., grad_fn=<MseLossBackward0>)\n",
      "tensor(2551913.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(2506656.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(2463621.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(2422657., grad_fn=<MseLossBackward0>)\n",
      "tensor(2383610., grad_fn=<MseLossBackward0>)\n",
      "tensor(2346393.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(2310901.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(2277076.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(2244790., grad_fn=<MseLossBackward0>)\n",
      "tensor(2213991.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(2184584.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(2156485., grad_fn=<MseLossBackward0>)\n",
      "tensor(2129632.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(2103974., grad_fn=<MseLossBackward0>)\n",
      "tensor(2079485., grad_fn=<MseLossBackward0>)\n",
      "tensor(2056070.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(2033694.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(2012293.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1991781.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1972144.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(1953317.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(1935242.6250, grad_fn=<MseLossBackward0>)\n",
      "tensor(1917859.6250, grad_fn=<MseLossBackward0>)\n",
      "tensor(1901132., grad_fn=<MseLossBackward0>)\n",
      "tensor(1885020.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(1869506.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(1854595.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(1840222.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1826408., grad_fn=<MseLossBackward0>)\n",
      "tensor(1813106.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(1800286.3750, grad_fn=<MseLossBackward0>)\n",
      "tensor(1787932.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(1776031.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(1764535.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(1753430., grad_fn=<MseLossBackward0>)\n",
      "tensor(1742703.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1732342.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(1722334.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1712646.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1703239.6250, grad_fn=<MseLossBackward0>)\n",
      "tensor(1694082.3750, grad_fn=<MseLossBackward0>)\n",
      "tensor(1685184.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(1676529.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1668121.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(1659899., grad_fn=<MseLossBackward0>)\n",
      "tensor(1651873.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(1644010.6250, grad_fn=<MseLossBackward0>)\n",
      "tensor(1636350.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(1628840.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1621483.3750, grad_fn=<MseLossBackward0>)\n",
      "tensor(1614282.3750, grad_fn=<MseLossBackward0>)\n",
      "tensor(1607233., grad_fn=<MseLossBackward0>)\n",
      "tensor(1600344.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1593583.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(1586982.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(1580533.1250, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.from_numpy(scaled_x_train).float()\n",
    "y = torch.from_numpy(y_train.values.reshape(-1, 1)).float()\n",
    "\n",
    "inputs = Variable(x)\n",
    "targets = Variable(y)\n",
    "\n",
    "base = base_model(1, [20])\n",
    "print(base)\n",
    "optimizer = torch.optim.Adagrad(base.parameters())\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "for i in range(100000):\n",
    "   prediction = base(inputs)\n",
    "   loss_base = loss_func(prediction, targets)\n",
    "   if i % 1000 == 0:\n",
    "      print(loss_base)\n",
    "   optimizer.zero_grad()\n",
    "   loss_base.backward()\n",
    "   optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1287.6952"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline loss\n",
    "test_x = Variable(torch.from_numpy(scaled_x_test).float())\n",
    "test_y = Variable(torch.from_numpy(y_test.values.reshape(-1, 1)).float())\n",
    "test_predictions_base = base(test_x)\n",
    "loss_base = loss_func(test_predictions_base, test_y)\n",
    "baseline_rmse = np.sqrt(loss_base.detach().numpy())\n",
    "baseline_rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model(\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=25, out_features=46, bias=True)\n",
      "    (1): Linear(in_features=46, out_features=96, bias=True)\n",
      "    (2): Linear(in_features=96, out_features=1, bias=True)\n",
      "  )\n",
      "  (activation): ReLU()\n",
      ")\n",
      "tensor(9496811., grad_fn=<MseLossBackward0>)\n",
      "tensor(1129050.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(937320.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(852728.3750, grad_fn=<MseLossBackward0>)\n",
      "tensor(795631.1875, grad_fn=<MseLossBackward0>)\n",
      "tensor(753532.3125, grad_fn=<MseLossBackward0>)\n",
      "tensor(717536.4375, grad_fn=<MseLossBackward0>)\n",
      "tensor(685593.3750, grad_fn=<MseLossBackward0>)\n",
      "tensor(658511.3125, grad_fn=<MseLossBackward0>)\n",
      "tensor(635521.3125, grad_fn=<MseLossBackward0>)\n",
      "tensor(615799.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(597967.8125, grad_fn=<MseLossBackward0>)\n",
      "tensor(581622.6250, grad_fn=<MseLossBackward0>)\n",
      "tensor(565580.6250, grad_fn=<MseLossBackward0>)\n",
      "tensor(549269., grad_fn=<MseLossBackward0>)\n",
      "tensor(533664.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(518780.6250, grad_fn=<MseLossBackward0>)\n",
      "tensor(504076.4375, grad_fn=<MseLossBackward0>)\n",
      "tensor(490110.2812, grad_fn=<MseLossBackward0>)\n",
      "tensor(476928., grad_fn=<MseLossBackward0>)\n",
      "tensor(464285.5625, grad_fn=<MseLossBackward0>)\n",
      "tensor(452330.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(441091.0625, grad_fn=<MseLossBackward0>)\n",
      "tensor(430215.5938, grad_fn=<MseLossBackward0>)\n",
      "tensor(419462.2188, grad_fn=<MseLossBackward0>)\n",
      "tensor(408671.6250, grad_fn=<MseLossBackward0>)\n",
      "tensor(398193.4688, grad_fn=<MseLossBackward0>)\n",
      "tensor(388638.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(379783.6875, grad_fn=<MseLossBackward0>)\n",
      "tensor(371504.3438, grad_fn=<MseLossBackward0>)\n",
      "tensor(363460.1875, grad_fn=<MseLossBackward0>)\n",
      "tensor(355493.9688, grad_fn=<MseLossBackward0>)\n",
      "tensor(347935.0938, grad_fn=<MseLossBackward0>)\n",
      "tensor(340776.3438, grad_fn=<MseLossBackward0>)\n",
      "tensor(333569.0625, grad_fn=<MseLossBackward0>)\n",
      "tensor(326902.5312, grad_fn=<MseLossBackward0>)\n",
      "tensor(320630.8125, grad_fn=<MseLossBackward0>)\n",
      "tensor(314631.2188, grad_fn=<MseLossBackward0>)\n",
      "tensor(308713.7812, grad_fn=<MseLossBackward0>)\n",
      "tensor(303091.4688, grad_fn=<MseLossBackward0>)\n",
      "tensor(297835.6250, grad_fn=<MseLossBackward0>)\n",
      "tensor(292968.9062, grad_fn=<MseLossBackward0>)\n",
      "tensor(288472., grad_fn=<MseLossBackward0>)\n",
      "tensor(284299.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(280069.5938, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.from_numpy(scaled_x_train).float()\n",
    "y = torch.from_numpy(y_train.values.reshape(-1, 1)).float()\n",
    "\n",
    "inputs = Variable(x)\n",
    "targets = Variable(y)\n",
    "\n",
    "best = base_model(best_params_dict[\"n_layers\"], \n",
    "                  [46, 96],\n",
    "                  # best_params_dict[\"n_hidden_units\"], \n",
    "                  # activation=best_params_dict[\"act_function\"],\n",
    "                  p=best_params_dict[\"dropout\"]\n",
    ")\n",
    "print(best)\n",
    "optimizer = torch.optim.Adagrad(best.parameters(), lr=best_params_dict[\"lr\"])\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "for i in range(best_params_dict[\"n_epochs\"]):\n",
    "   prediction = best(inputs)\n",
    "   loss = loss_func(prediction, targets)\n",
    "   if i % 1000 == 0:\n",
    "      print(loss)\n",
    "   optimizer.zero_grad()\n",
    "   loss.backward()\n",
    "   optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "852.6837"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best model loss\n",
    "test_predictions = best(test_x)\n",
    "loss = loss_func(test_predictions, test_y)\n",
    "best_rmse = np.sqrt(loss.detach().numpy())\n",
    "best_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_test_r2(pred_vals, true_vals, baseline_rmse):\n",
    "    sse = mean_squared_error(pred_vals, true_vals) * len(true_vals)\n",
    "    sst = (baseline_rmse ** 2) * len(true_vals)\n",
    "    return (\n",
    "        1 - sse / sst, np.sqrt(sse / len(true_vals)),\n",
    "        mean_absolute_error(pred_vals, true_vals)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7087489089518462, 852.6837065281679, 600.8200379971026)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_test_r2(test_predictions.detach().numpy(), y_test.values.reshape(-1, 1), np.sqrt(np.var(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_np = test_predictions.detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18766839.538250506"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max((test_predictions_np - y_test.values.reshape(-1, 1)) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "largest_error = np.argmax((test_predictions_np - y_test.values.reshape(-1, 1)) ** 2)\n",
    "largest_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "circuit_name                  WYANDOTTE\n",
       "deenergize_time     2020-09-07 22:20:00\n",
       "restoration_time    2020-09-13 08:52:00\n",
       "key_communities                OROVILLE\n",
       "hftd_tier                           3.0\n",
       "                           ...         \n",
       "zip_is_96073                        0.0\n",
       "zip_is_96076                        0.0\n",
       "zip_is_96080                        0.0\n",
       "zip_is_96096                        0.0\n",
       "zip_is_96137                        0.0\n",
       "Name: 1221, Length: 285, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.iloc[largest_error, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
