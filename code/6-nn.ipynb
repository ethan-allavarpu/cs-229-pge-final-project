{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x168b47f30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchviz import make_dot\n",
    "\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv(\n",
    "    \"../data/processed/x_train_w_OHE.csv\", index_col=0, dtype=str\n",
    ")\n",
    "x_test = pd.read_csv(\n",
    "    \"../data/processed/x_test_w_OHE.csv\", index_col=0, dtype=str\n",
    ")\n",
    "y_train = pd.read_csv(\n",
    "    \"../data/processed/y_train.csv\", index_col=0, dtype=float\n",
    ").squeeze(\"columns\").reset_index(drop=True)\n",
    "y_test = pd.read_csv(\n",
    "    \"../data/processed/y_test.csv\", index_col=0, dtype=float\n",
    ").squeeze(\"columns\").reset_index(drop=True)\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "zip_cols = x_train.columns[\n",
    "    [re.search('zip_is', col) is not None for col in x_train.columns]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_types_x(df, numeric_cols):\n",
    "    for col in ['deenergize_time', 'restoration_time']:\n",
    "        df[col] = pd.to_datetime(df[col], format='%Y-%m-%d %H:%M:%S')\n",
    "    for col in numeric_cols:\n",
    "        df[col] = df[col].astype(float)\n",
    "    return df\n",
    "\n",
    "\n",
    "numeric_cols = [\n",
    "    'hftd_tier', 'total_affected', 'residential_affected',\n",
    "    'longitude', 'latitude', 'total_pop', 'median_age', 'median_income',\n",
    "    'white_pct', 'tmin_d-5', 'tmax_d-5', 'wspd_d-5', 'tmin_d-4', 'tmax_d-4',\n",
    "    'wspd_d-4', 'tmin_d-3', 'tmax_d-3', 'wspd_d-3', 'tmin_d-2', 'tmax_d-2',\n",
    "    'wspd_d-2', 'tmin_d-1', 'tmax_d-1', 'wspd_d-1', 'day_in_year'\n",
    "]\n",
    "x_train = get_correct_types_x(x_train, numeric_cols)\n",
    "x_valid = get_correct_types_x(x_valid, numeric_cols)\n",
    "x_test = get_correct_types_x(x_test, numeric_cols)\n",
    "rel_x_train = x_train[numeric_cols]\n",
    "rel_x_valid = x_valid[numeric_cols]\n",
    "rel_x_test = x_test[numeric_cols]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(rel_x_train)\n",
    "scaled_x_train = scaler.transform(rel_x_train)\n",
    "scaled_x_valid = scaler.transform(rel_x_valid)\n",
    "scaled_x_test = scaler.transform(rel_x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class base_model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_hidden_layers, n_hidden_units, p=0.1, activation=torch.nn.ReLU()):\n",
    "        super(base_model, self).__init__()\n",
    "        if n_hidden_layers == 0:\n",
    "            self.linears = torch.nn.ModuleList([\n",
    "                torch.nn.Linear(scaled_x_train.shape[1], 1)\n",
    "            ])\n",
    "            self.activation = activation\n",
    "            self.dropout = torch.nn.Dropout(p)\n",
    "        else:\n",
    "            assert len(n_hidden_units) == n_hidden_layers\n",
    "            self.layers = []\n",
    "\n",
    "            for layer, n_units in enumerate(n_hidden_units):\n",
    "                if layer == 0:\n",
    "                    curr_layer = torch.nn.Linear(\n",
    "                        scaled_x_train.shape[1], n_units)\n",
    "                else:\n",
    "                    curr_layer = torch.nn.Linear(\n",
    "                        n_hidden_units[layer - 1], n_units)\n",
    "                self.layers.append(curr_layer)\n",
    "            self.layers.append(torch.nn.Linear(n_hidden_units[-1], 1))\n",
    "            self.linears = torch.nn.ModuleList(self.layers)\n",
    "            self.activation = activation\n",
    "            self.dropout = torch.nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.linears:\n",
    "            x = self.dropout(self.activation(layer(x)))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter Optimization using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. Define an objective function to be maximized.\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    # 2. Suggest values of the hyperparameters using a trial object.\n",
    "    n_layers = trial.suggest_int('n_layers', 0, 4)\n",
    "    n_hidden_units = [0] * n_layers\n",
    "    print(n_layers)\n",
    "    for i in range(n_layers):\n",
    "        n_hidden_units[i] = trial.suggest_int(f\"n_h_{i}\", 1, 50)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 5e-1, log=True)\n",
    "    n_epochs = trial.suggest_int(\"n_epochs\", 1000, 100000)\n",
    "    activation_functions = [torch.nn.ReLU(), torch.nn.Tanh()]\n",
    "    act_idx = trial.suggest_categorical(\"act_function\", [0, 1])\n",
    "    dropout_p = trial.suggest_float(\"dropout\", 0, 1)\n",
    "    params = f\"\"\"Params:\n",
    "          n_layers: {n_layers}\n",
    "          n_hidden_units: {n_hidden_units}\n",
    "          lr: {lr}\n",
    "          n_epochs: {n_epochs}\n",
    "          act_function: {activation_functions[act_idx]}\n",
    "          dropout: {dropout_p}\"\"\"\n",
    "    print(params)\n",
    "\n",
    "    x = torch.from_numpy(scaled_x_train).float().to(device)\n",
    "    y = torch.from_numpy(y_train.values.reshape(-1, 1)).float().to(device)\n",
    "\n",
    "    inputs = Variable(x)\n",
    "    targets = Variable(y)\n",
    "\n",
    "    # base = base_model(1, [1], activation=torch.nn.Tanh())\n",
    "    base = base_model(n_layers, n_hidden_units, p=dropout_p,\n",
    "                      activation=activation_functions[act_idx])\n",
    "    base.to(device)\n",
    "    optimizer = torch.optim.Adagrad(base.parameters(), lr=lr)\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    prev_loss = torch.from_numpy(np.array([0])).to(device)\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        prediction = base(inputs)\n",
    "        loss = loss_func(prediction, targets)\n",
    "        if i % 1000 == 0:\n",
    "            print(loss)\n",
    "        if np.abs(loss.cpu().detach().numpy() - prev_loss.cpu().detach().numpy()) < 1e-8:\n",
    "          break\n",
    "        prev_loss = loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    valid_x = Variable(torch.from_numpy(scaled_x_valid).float()).to(device)\n",
    "    valid_y = Variable(torch.from_numpy(\n",
    "        y_valid.values.reshape(-1, 1)).float()).to(device)\n",
    "    valid_predictions = base(valid_x)\n",
    "    loss = loss_func(valid_predictions, valid_y)\n",
    "    print(f\"Final valid loss: {loss}\")\n",
    "    print(\"#################\")\n",
    "    if loss < 1000000:\n",
    "      with open(f\"gdrive/MyDrive/CS229_Final_Project/nn_hpo/run_2/{time.time()}.txt\", \"w+\") as f:\n",
    "        f.write(f\"Loss: {np.sqrt(loss.cpu().detach().numpy())}\\n\")\n",
    "        f.write(params)\n",
    "    return np.sqrt(loss.cpu().detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create a study object and optimize the objective function.\n",
    "torch.manual_seed(0)\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "pd.DataFrame.from_dict({\"value\": study.best_trial.values, \"params\": str(\n",
    "    study.best_trial.params)}).to_csv(\"gdrive/MyDrive/CS229_Final_Project/nn_hpo/run_4.csv\", index=False)\n",
    "\n",
    "# fig = optuna.visualization.plot_optimization_history(study)\n",
    "# fig.show()\n",
    "# fig.write_image(\"gdrive/MyDrive/CS229_Final_Project/nn_hpo/run_1.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running best models on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_layers': 2,\n",
       " 'n_h_0': 46,\n",
       " 'n_h_1': 96,\n",
       " 'lr': 0.011578444404576697,\n",
       " 'n_epochs': 44718,\n",
       " 'act_function': 0,\n",
       " 'dropout': 0.05}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load best hyperparams\n",
    "best_params = pd.read_csv(\"nn_hpo/run_1.csv\")\n",
    "best_params_dict = eval(best_params[\"params\"].values[0])\n",
    "best_params_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model(\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=25, out_features=20, bias=True)\n",
      "    (1): Linear(in_features=20, out_features=1, bias=True)\n",
      "  )\n",
      "  (activation): ReLU()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "tensor(9496955., grad_fn=<MseLossBackward0>)\n",
      "tensor(9170094., grad_fn=<MseLossBackward0>)\n",
      "tensor(8842019., grad_fn=<MseLossBackward0>)\n",
      "tensor(8544570., grad_fn=<MseLossBackward0>)\n",
      "tensor(8241723.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(7971185.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(7695174., grad_fn=<MseLossBackward0>)\n",
      "tensor(7436797.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(7203096.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(6954623., grad_fn=<MseLossBackward0>)\n",
      "tensor(6749131.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(6529509., grad_fn=<MseLossBackward0>)\n",
      "tensor(6462547., grad_fn=<MseLossBackward0>)\n",
      "tensor(6162862.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(6082063.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(5940748., grad_fn=<MseLossBackward0>)\n",
      "tensor(5691318., grad_fn=<MseLossBackward0>)\n",
      "tensor(5500298.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(5429348.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(5259335., grad_fn=<MseLossBackward0>)\n",
      "tensor(5129413.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(5136102.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(4881572., grad_fn=<MseLossBackward0>)\n",
      "tensor(4791630.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(4734566.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(4789864., grad_fn=<MseLossBackward0>)\n",
      "tensor(4485553.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(4316379., grad_fn=<MseLossBackward0>)\n",
      "tensor(4430706., grad_fn=<MseLossBackward0>)\n",
      "tensor(4070140.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(4149771., grad_fn=<MseLossBackward0>)\n",
      "tensor(4036003.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(4050594.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3948458.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(3827683., grad_fn=<MseLossBackward0>)\n",
      "tensor(3883114.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3860234.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(3726729.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3662656.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(3561690., grad_fn=<MseLossBackward0>)\n",
      "tensor(3429518.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3467203.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3311608., grad_fn=<MseLossBackward0>)\n",
      "tensor(3476857.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(3276782.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3236091.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3279750.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3318918.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3097643., grad_fn=<MseLossBackward0>)\n",
      "tensor(3085909., grad_fn=<MseLossBackward0>)\n",
      "tensor(3171781.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(3046183.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3049135.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3125017.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3063318.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3022874.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3032162.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(2932405.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(3125829.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(2833130.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(2900235.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(2893875.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(2901531.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(2775530.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(2727472.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(2751996.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(2803291.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(2651828.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(2557253.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(2807038.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(2752196., grad_fn=<MseLossBackward0>)\n",
      "tensor(2762990.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(2647651.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(2870140.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(2595499.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(2656743.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(2580215., grad_fn=<MseLossBackward0>)\n",
      "tensor(2620342.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(2586445.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(2445995.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(2637993.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(2621758.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(2603330., grad_fn=<MseLossBackward0>)\n",
      "tensor(2754692., grad_fn=<MseLossBackward0>)\n",
      "tensor(2771211.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(2562717., grad_fn=<MseLossBackward0>)\n",
      "tensor(2599397.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(2478348.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(2507672.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(2567233., grad_fn=<MseLossBackward0>)\n",
      "tensor(2337771.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(2570382.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(2458085., grad_fn=<MseLossBackward0>)\n",
      "tensor(2392276.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(2418423.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(2372992.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(2400329.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(2423868.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(2319198.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(2357880.5000, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.from_numpy(scaled_x_train).float()\n",
    "y = torch.from_numpy(y_train.values.reshape(-1, 1)).float()\n",
    "\n",
    "inputs = Variable(x)\n",
    "targets = Variable(y)\n",
    "\n",
    "base = base_model(1, [20])\n",
    "print(base)\n",
    "optimizer = torch.optim.Adagrad(base.parameters())\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "for i in range(100000):\n",
    "   prediction = base(inputs)\n",
    "   loss_base = loss_func(prediction, targets)\n",
    "   if i % 1000 == 0:\n",
    "      print(loss_base)\n",
    "   optimizer.zero_grad()\n",
    "   loss_base.backward()\n",
    "   optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1667.5623"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline loss\n",
    "test_x = Variable(torch.from_numpy(scaled_x_test).float())\n",
    "test_y = Variable(torch.from_numpy(y_test.values.reshape(-1, 1)).float())\n",
    "test_predictions_base = base(test_x)\n",
    "loss_base = loss_func(test_predictions_base, test_y)\n",
    "baseline_rmse = np.sqrt(loss_base.detach().numpy())\n",
    "baseline_rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model(\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=25, out_features=46, bias=True)\n",
      "    (1): Linear(in_features=46, out_features=96, bias=True)\n",
      "    (2): Linear(in_features=96, out_features=1, bias=True)\n",
      "  )\n",
      "  (activation): ReLU()\n",
      "  (dropout): Dropout(p=0.05, inplace=False)\n",
      ")\n",
      "tensor(9497148., grad_fn=<MseLossBackward0>)\n",
      "tensor(1571939.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1363098.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1276071.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(1295430.3750, grad_fn=<MseLossBackward0>)\n",
      "tensor(1293354.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1060833.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(1124013.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(1193367.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(1139124.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(1201476.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(1213103.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(1126465.6250, grad_fn=<MseLossBackward0>)\n",
      "tensor(1177625.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(1033953.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(913700., grad_fn=<MseLossBackward0>)\n",
      "tensor(1203706.3750, grad_fn=<MseLossBackward0>)\n",
      "tensor(1161980.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(1141211.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(990804.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(1281519.3750, grad_fn=<MseLossBackward0>)\n",
      "tensor(1117216.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(986329.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1007814.1875, grad_fn=<MseLossBackward0>)\n",
      "tensor(1182113.6250, grad_fn=<MseLossBackward0>)\n",
      "tensor(1031847.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1286771.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(876793.6875, grad_fn=<MseLossBackward0>)\n",
      "tensor(977028.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(894048.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(1056535.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(902812., grad_fn=<MseLossBackward0>)\n",
      "tensor(926535.0625, grad_fn=<MseLossBackward0>)\n",
      "tensor(968842.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(969424.3125, grad_fn=<MseLossBackward0>)\n",
      "tensor(868573.5625, grad_fn=<MseLossBackward0>)\n",
      "tensor(1033686.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(1214388.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(841553.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(1059966.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(865334.6250, grad_fn=<MseLossBackward0>)\n",
      "tensor(947989.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(931014.9375, grad_fn=<MseLossBackward0>)\n",
      "tensor(857829.9375, grad_fn=<MseLossBackward0>)\n",
      "tensor(1136826.1250, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.from_numpy(scaled_x_train).float()\n",
    "y = torch.from_numpy(y_train.values.reshape(-1, 1)).float()\n",
    "\n",
    "inputs = Variable(x)\n",
    "targets = Variable(y)\n",
    "\n",
    "best = base_model(best_params_dict[\"n_layers\"], \n",
    "                  [46, 96],\n",
    "                  # best_params_dict[\"n_hidden_units\"], \n",
    "                  # activation=best_params_dict[\"act_function\"],\n",
    "                  p=best_params_dict[\"dropout\"]\n",
    ")\n",
    "print(best)\n",
    "optimizer = torch.optim.Adagrad(best.parameters(), lr=best_params_dict[\"lr\"])\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "for i in range(best_params_dict[\"n_epochs\"]):\n",
    "   prediction = best(inputs)\n",
    "   loss = loss_func(prediction, targets)\n",
    "   if i % 1000 == 0:\n",
    "      print(loss)\n",
    "   optimizer.zero_grad()\n",
    "   loss.backward()\n",
    "   optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1095.2943"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best model loss\n",
    "test_predictions = best(test_x)\n",
    "loss = loss_func(test_predictions, test_y)\n",
    "best_rmse = np.sqrt(loss.detach().numpy())\n",
    "best_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.50.0 (0)\n -->\n<!-- Pages: 1 -->\n<svg width=\"353pt\" height=\"578pt\"\n viewBox=\"0.00 0.00 353.00 578.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 574)\">\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-574 349,-574 349,4 -4,4\"/>\n<!-- 6269979520 -->\n<g id=\"node1\" class=\"node\">\n<title>6269979520</title>\n<polygon fill=\"#caff70\" stroke=\"black\" points=\"202,-31 131,-31 131,0 202,0 202,-31\"/>\n<text text-anchor=\"middle\" x=\"166.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (393, 1)</text>\n</g>\n<!-- 10785268304 -->\n<g id=\"node2\" class=\"node\">\n<title>10785268304</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"211,-86 122,-86 122,-67 211,-67 211,-86\"/>\n<text text-anchor=\"middle\" x=\"166.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n</g>\n<!-- 10785268304&#45;&gt;6269979520 -->\n<g id=\"edge16\" class=\"edge\">\n<title>10785268304&#45;&gt;6269979520</title>\n<path fill=\"none\" stroke=\"black\" d=\"M166.5,-66.79C166.5,-60.07 166.5,-50.4 166.5,-41.34\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"170,-41.19 166.5,-31.19 163,-41.19 170,-41.19\"/>\n</g>\n<!-- 10785268688 -->\n<g id=\"node3\" class=\"node\">\n<title>10785268688</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"214,-141 119,-141 119,-122 214,-122 214,-141\"/>\n<text text-anchor=\"middle\" x=\"166.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n</g>\n<!-- 10785268688&#45;&gt;10785268304 -->\n<g id=\"edge1\" class=\"edge\">\n<title>10785268688&#45;&gt;10785268304</title>\n<path fill=\"none\" stroke=\"black\" d=\"M166.5,-121.75C166.5,-114.8 166.5,-104.85 166.5,-96.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"170,-96.09 166.5,-86.09 163,-96.09 170,-96.09\"/>\n</g>\n<!-- 10785268208 -->\n<g id=\"node4\" class=\"node\">\n<title>10785268208</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"217,-196 116,-196 116,-177 217,-177 217,-196\"/>\n<text text-anchor=\"middle\" x=\"166.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n</g>\n<!-- 10785268208&#45;&gt;10785268688 -->\n<g id=\"edge2\" class=\"edge\">\n<title>10785268208&#45;&gt;10785268688</title>\n<path fill=\"none\" stroke=\"black\" d=\"M166.5,-176.75C166.5,-169.8 166.5,-159.85 166.5,-151.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"170,-151.09 166.5,-141.09 163,-151.09 170,-151.09\"/>\n</g>\n<!-- 10785254368 -->\n<g id=\"node5\" class=\"node\">\n<title>10785254368</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"102,-251 1,-251 1,-232 102,-232 102,-251\"/>\n<text text-anchor=\"middle\" x=\"51.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 10785254368&#45;&gt;10785268208 -->\n<g id=\"edge3\" class=\"edge\">\n<title>10785254368&#45;&gt;10785268208</title>\n<path fill=\"none\" stroke=\"black\" d=\"M69.98,-231.98C88.48,-223.46 117.18,-210.23 138.48,-200.41\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"140.07,-203.53 147.69,-196.17 137.14,-197.18 140.07,-203.53\"/>\n</g>\n<!-- 6268996800 -->\n<g id=\"node6\" class=\"node\">\n<title>6268996800</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"101,-317 0,-317 0,-287 101,-287 101,-317\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-305\" font-family=\"monospace\" font-size=\"10.00\">linears.1.bias</text>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n</g>\n<!-- 6268996800&#45;&gt;10785254368 -->\n<g id=\"edge4\" class=\"edge\">\n<title>6268996800&#45;&gt;10785254368</title>\n<path fill=\"none\" stroke=\"black\" d=\"M50.74,-286.84C50.87,-279.21 51.03,-269.7 51.18,-261.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"54.68,-261.32 51.35,-251.27 47.68,-261.2 54.68,-261.32\"/>\n</g>\n<!-- 10785254560 -->\n<g id=\"node7\" class=\"node\">\n<title>10785254560</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"211,-251 122,-251 122,-232 211,-232 211,-251\"/>\n<text text-anchor=\"middle\" x=\"166.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n</g>\n<!-- 10785254560&#45;&gt;10785268208 -->\n<g id=\"edge5\" class=\"edge\">\n<title>10785254560&#45;&gt;10785268208</title>\n<path fill=\"none\" stroke=\"black\" d=\"M166.5,-231.75C166.5,-224.8 166.5,-214.85 166.5,-206.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"170,-206.09 166.5,-196.09 163,-206.09 170,-206.09\"/>\n</g>\n<!-- 10785254752 -->\n<g id=\"node8\" class=\"node\">\n<title>10785254752</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"214,-311.5 119,-311.5 119,-292.5 214,-292.5 214,-311.5\"/>\n<text text-anchor=\"middle\" x=\"166.5\" y=\"-299.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n</g>\n<!-- 10785254752&#45;&gt;10785254560 -->\n<g id=\"edge6\" class=\"edge\">\n<title>10785254752&#45;&gt;10785254560</title>\n<path fill=\"none\" stroke=\"black\" d=\"M166.5,-292.37C166.5,-284.25 166.5,-271.81 166.5,-261.39\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"170,-261.17 166.5,-251.17 163,-261.17 170,-261.17\"/>\n</g>\n<!-- 10785253792 -->\n<g id=\"node9\" class=\"node\">\n<title>10785253792</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"214,-377.5 113,-377.5 113,-358.5 214,-358.5 214,-377.5\"/>\n<text text-anchor=\"middle\" x=\"163.5\" y=\"-365.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n</g>\n<!-- 10785253792&#45;&gt;10785254752 -->\n<g id=\"edge7\" class=\"edge\">\n<title>10785253792&#45;&gt;10785254752</title>\n<path fill=\"none\" stroke=\"black\" d=\"M163.9,-358.37C164.34,-349.07 165.05,-333.98 165.61,-321.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"169.11,-322.06 166.08,-311.91 162.12,-321.73 169.11,-322.06\"/>\n</g>\n<!-- 10785253984 -->\n<g id=\"node10\" class=\"node\">\n<title>10785253984</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"161,-438 60,-438 60,-419 161,-419 161,-438\"/>\n<text text-anchor=\"middle\" x=\"110.5\" y=\"-426\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 10785253984&#45;&gt;10785253792 -->\n<g id=\"edge8\" class=\"edge\">\n<title>10785253984&#45;&gt;10785253792</title>\n<path fill=\"none\" stroke=\"black\" d=\"M118.32,-418.87C126.31,-410.05 138.89,-396.16 148.76,-385.27\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"151.53,-387.43 155.65,-377.67 146.34,-382.73 151.53,-387.43\"/>\n</g>\n<!-- 10784992608 -->\n<g id=\"node11\" class=\"node\">\n<title>10784992608</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"155,-504 54,-504 54,-474 155,-474 155,-504\"/>\n<text text-anchor=\"middle\" x=\"104.5\" y=\"-492\" font-family=\"monospace\" font-size=\"10.00\">linears.0.bias</text>\n<text text-anchor=\"middle\" x=\"104.5\" y=\"-481\" font-family=\"monospace\" font-size=\"10.00\"> (20)</text>\n</g>\n<!-- 10784992608&#45;&gt;10785253984 -->\n<g id=\"edge9\" class=\"edge\">\n<title>10784992608&#45;&gt;10785253984</title>\n<path fill=\"none\" stroke=\"black\" d=\"M105.95,-473.84C106.73,-466.21 107.71,-456.7 108.56,-448.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"112.06,-448.57 109.6,-438.27 105.1,-447.86 112.06,-448.57\"/>\n</g>\n<!-- 10785252400 -->\n<g id=\"node12\" class=\"node\">\n<title>10785252400</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"259,-438 182,-438 182,-419 259,-419 259,-438\"/>\n<text text-anchor=\"middle\" x=\"220.5\" y=\"-426\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n</g>\n<!-- 10785252400&#45;&gt;10785253792 -->\n<g id=\"edge10\" class=\"edge\">\n<title>10785252400&#45;&gt;10785253792</title>\n<path fill=\"none\" stroke=\"black\" d=\"M212.09,-418.87C203.42,-409.97 189.7,-395.89 179.04,-384.95\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"181.43,-382.39 171.95,-377.67 176.42,-387.27 181.43,-382.39\"/>\n</g>\n<!-- 10785254032 -->\n<g id=\"node13\" class=\"node\">\n<title>10785254032</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"274,-498.5 173,-498.5 173,-479.5 274,-479.5 274,-498.5\"/>\n<text text-anchor=\"middle\" x=\"223.5\" y=\"-486.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 10785254032&#45;&gt;10785252400 -->\n<g id=\"edge11\" class=\"edge\">\n<title>10785254032&#45;&gt;10785252400</title>\n<path fill=\"none\" stroke=\"black\" d=\"M223.06,-479.37C222.64,-471.25 222,-458.81 221.47,-448.39\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"224.95,-447.97 220.94,-438.17 217.96,-448.33 224.95,-447.97\"/>\n</g>\n<!-- 10784771024 -->\n<g id=\"node14\" class=\"node\">\n<title>10784771024</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"280,-570 167,-570 167,-540 280,-540 280,-570\"/>\n<text text-anchor=\"middle\" x=\"223.5\" y=\"-558\" font-family=\"monospace\" font-size=\"10.00\">linears.0.weight</text>\n<text text-anchor=\"middle\" x=\"223.5\" y=\"-547\" font-family=\"monospace\" font-size=\"10.00\"> (20, 25)</text>\n</g>\n<!-- 10784771024&#45;&gt;10785254032 -->\n<g id=\"edge12\" class=\"edge\">\n<title>10784771024&#45;&gt;10785254032</title>\n<path fill=\"none\" stroke=\"black\" d=\"M223.5,-539.8C223.5,-530.7 223.5,-518.79 223.5,-508.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"227,-508.84 223.5,-498.84 220,-508.84 227,-508.84\"/>\n</g>\n<!-- 10785254128 -->\n<g id=\"node15\" class=\"node\">\n<title>10785254128</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"313,-251 236,-251 236,-232 313,-232 313,-251\"/>\n<text text-anchor=\"middle\" x=\"274.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n</g>\n<!-- 10785254128&#45;&gt;10785268208 -->\n<g id=\"edge13\" class=\"edge\">\n<title>10785254128&#45;&gt;10785268208</title>\n<path fill=\"none\" stroke=\"black\" d=\"M257.15,-231.98C239.93,-223.54 213.3,-210.47 193.35,-200.68\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"194.69,-197.43 184.17,-196.17 191.6,-203.72 194.69,-197.43\"/>\n</g>\n<!-- 10785253696 -->\n<g id=\"node16\" class=\"node\">\n<title>10785253696</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"335,-311.5 234,-311.5 234,-292.5 335,-292.5 335,-311.5\"/>\n<text text-anchor=\"middle\" x=\"284.5\" y=\"-299.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 10785253696&#45;&gt;10785254128 -->\n<g id=\"edge14\" class=\"edge\">\n<title>10785253696&#45;&gt;10785254128</title>\n<path fill=\"none\" stroke=\"black\" d=\"M283.02,-292.37C281.62,-284.16 279.46,-271.54 277.67,-261.05\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"281.12,-260.43 275.98,-251.17 274.22,-261.61 281.12,-260.43\"/>\n</g>\n<!-- 6268996720 -->\n<g id=\"node17\" class=\"node\">\n<title>6268996720</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"345,-383 232,-383 232,-353 345,-353 345,-383\"/>\n<text text-anchor=\"middle\" x=\"288.5\" y=\"-371\" font-family=\"monospace\" font-size=\"10.00\">linears.1.weight</text>\n<text text-anchor=\"middle\" x=\"288.5\" y=\"-360\" font-family=\"monospace\" font-size=\"10.00\"> (1, 20)</text>\n</g>\n<!-- 6268996720&#45;&gt;10785253696 -->\n<g id=\"edge15\" class=\"edge\">\n<title>6268996720&#45;&gt;10785253696</title>\n<path fill=\"none\" stroke=\"black\" d=\"M287.61,-352.8C287.04,-343.7 286.3,-331.79 285.68,-321.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"289.17,-321.61 285.05,-311.84 282.18,-322.04 289.17,-321.61\"/>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x282da2160>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(test_predictions_base, params=dict(base.named_parameters()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_test_r2(pred_vals, true_vals, baseline_rmse):\n",
    "    sse = mean_squared_error(pred_vals, true_vals) * len(true_vals)\n",
    "    sst = (baseline_rmse ** 2) * len(true_vals)\n",
    "    return (\n",
    "        1 - sse / sst, \n",
    "        np.sqrt(sse / len(true_vals)),\n",
    "        mean_absolute_error(pred_vals, true_vals),\n",
    "        mean_absolute_percentage_error(pred_vals, true_vals)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5194337065600544,\n",
       " 1095.2943240786306,\n",
       " 738.6903584021649,\n",
       " 5.835656677460945e+17)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_test_r2(test_predictions.detach().numpy(), y_test.values.reshape(-1, 1), np.sqrt(np.var(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95ef66565be0ab37caf8d705f94b2ba6fd3ff1d5242a0930694c635ab854b36c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
